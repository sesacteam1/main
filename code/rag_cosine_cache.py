# ì½”ì‚¬ì¸ìœ ì‚¬ë„ / # ë‚´ìš©ì˜ íë¦„ ì¼ì¹˜ ê²€ì¦

# ê°œë°œ í™˜ê²½
# Python 3.11
import logging
from dotenv import load_dotenv
import os
from datetime import datetime
import time


# DB
import pymysql
import faiss
import numpy as np

# ChatBot
import openai
import streamlit as st

import json

# LangChain
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate


from langchain.text_splitter import RecursiveCharacterTextSplitter
import locale


# ë¡œê·¸ ì„¤ì •
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# MySQL ì—°ê²° ì •ë³´
MYSQL_USER = os.getenv("MYSQL_USER")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD")

log_print = []

# âœ… 1ï¸âƒ£ ì§ˆë¬¸ ë¶„ë¥˜ GPT
def classify_question(question):
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ” GPT"""
    logging.info(f"ğŸŸ¡ [ì§ˆë¬¸ ë¶„ë¥˜] ì…ë ¥ ì§ˆë¬¸: {question}")
    log_print.append(f"{timestamp}ğŸŸ¡ [ì§ˆë¬¸ ë¶„ë¥˜] ì…ë ¥ ì§ˆë¬¸: {question}")

    system_prompt = """
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤.
    
    ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬:
    1. ì±„ìš© ì •ë³´
    2. ì¼ë°˜ ì§ˆë¬¸
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— 'ìš”ì²­'ì´ í¬í•¨ë˜ì§€ ì•Šì•˜ì„ì‹œ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

    **ì¶œë ¥ í˜•ì‹ (JSON)**
    {"category": "ì±„ìš© ì •ë³´"}
    """

    ####ì‚¬ëŒë“¤ì˜ ë¡œê·¸ë°ì´í„° ë¶„ì„ì„ í†µí•´ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„¸ë¶„í™” í•˜ê² ë‹¤~. ë°°íƒ€ì ì´ì—¬ì•¼í•¨.

    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ],
        temperature=0.3,
        max_tokens=50
    )

    result = json.loads(response["choices"][0]["message"]["content"])
    logging.info(f"ğŸŸ¢ [ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ] ì¹´í…Œê³ ë¦¬: {result['category']}")
    return result  # JSON ë³€í™˜

# âœ… 2ï¸âƒ£ ë°ì´í„° ê²€ìƒ‰ GPT
def connect_to_db():
    """MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    return pymysql.connect(
        host='127.0.0.1',
        port=3306,
        user=MYSQL_USER,
        passwd=MYSQL_PASSWORD,
        db='sesacteam1',
        charset="utf8mb4"
    )

@st.cache_data
def fetch_from_saramin_refined():
    """MySQLì—ì„œ ì±„ìš© ì •ë³´ë¥¼ ê°€ì ¸ì˜´"""
    connection = connect_to_db()
    cursor = connection.cursor(pymysql.cursors.DictCursor)
    cursor.execute("SELECT * FROM saramin_5_combined_revised_fordb")
    fetch_result = cursor.fetchall()
    connection.close()
    
    logging.info(f"ğŸŸ¢ [DB ë°ì´í„° ë¡œë“œ ì™„ë£Œ] ì´ {len(fetch_result)}ê°œ ë¬¸ì„œ ê°€ì ¸ì˜´")
    return fetch_result

@st.cache_data
def convert_to_documents(courses):
    """MySQL ë°ì´í„°ë¥¼ LangChain ë¬¸ì„œë¡œ ë³€í™˜ (ì²­í¬ ì—†ìŒ)"""
    documents = []
    for course in courses:
        doc_text = "\n".join([f"{key}: {value}" for key, value in course.items()])
        documents.append(Document(page_content=doc_text))  # ì²­í¬ ì—†ì´ ì „ì²´ ë¬¸ì„œ ì €ì¥

    logging.info(f"ğŸŸ¢ [ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ] ì´ {len(documents)}ê°œ ë¬¸ì„œ ìƒì„±ë¨")
    return documents

def normalize_vector_store(store):
    """FAISS ë²¡í„°ë“¤ì„ ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ê²Œ ë³€ê²½"""
    xb = store.index.reconstruct_n(0, store.index.ntotal)  # ëª¨ë“  ë²¡í„° ê°€ì ¸ì˜¤ê¸°
    xb /= np.linalg.norm(xb, axis=1, keepdims=True)  # ë²¡í„° ì •ê·œí™”
    
    # ì •ê·œí™”ëœ ë²¡í„°ì˜ L2 ë…¸ë¦„ í™•ì¸
    norm = np.linalg.norm(xb, axis=1)
    if np.allclose(norm, 1.0):
        print("ë²¡í„°ëŠ” ëª¨ë‘ ì •ê·œí™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ë²¡í„°ê°€ ì¼ë¶€ ì •ê·œí™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    store.index = faiss.IndexFlatIP(xb.shape[1])  # ë‚´ì  ê¸°ë°˜ ì¸ë±ìŠ¤ ë‹¤ì‹œ ìƒì„±
    store.index.add(xb)  # ì •ê·œí™”ëœ ë²¡í„° ì¶”ê°€
    # ìƒˆë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ì˜ metric_type ì¶œë ¥
    print("ìƒˆë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ì˜ metric_type:", store.index.metric_type)
    return store

@st.cache_resource
def create_vector_store(_documents):
    """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì €ì¥"""
    embeddings = OpenAIEmbeddings()
    faiss_path = './db/faiss_norm'  # ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ

    # âœ… ë²¡í„° ìŠ¤í† ì–´ê°€ ì¡´ì¬í•˜ë©´ ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(faiss_path):
        logging.info("ğŸŸ¡ [ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ] ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        store = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)
    else:
        # âŒ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ìƒì„± í›„ ì €ì¥
        logging.info("ğŸ”µ [ë²¡í„° ìŠ¤í† ì–´ ìƒì„±] ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë§Œë“­ë‹ˆë‹¤.")
        store = FAISS.from_documents(documents, embeddings)
        # ğŸ”¹ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìš©: ë²¡í„° ì •ê·œí™”
        store = normalize_vector_store(store)
        
        
        store.save_local(faiss_path)  # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        logging.info("ğŸŸ¢ [ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ] ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    logging.info(f"ğŸŸ¢ [ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ] ë¬¸ì„œ ê°œìˆ˜: {len(documents)}")
    
    return store

def create_chatbot(vector_store):
    """ì±„ìš© ì •ë³´ ê¸°ë°˜ ì±—ë´‡ ìƒì„±"""
    chat_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

    prompt_template = PromptTemplate(
        input_variables=["question", "context"],  # í•„ìš”í•œ ë³€ìˆ˜ ì •ì˜
        template="""
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:
        1. ë‹¹ì‹ ì€ ì¡Jobì˜ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤.ì°¸ì¡° ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ëŠ” ì±„ìš© ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
        2. ì±„ìš© ê³µê³ ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí• ì‹œ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. 'ê³µê³ ì œëª©', 'ê³µê³ ë§í¬', 'ê¸°ì—…ëª…'ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”. 
        - ê³µê³  ì œëª©: (ê³µê³ ì œëª©)
        - ê³µê³  ë§í¬: (ê³µê³ ë§í¬ë¥¼ ê·¸ëŒ€ë¡œ ì œê³µí•˜ì„¸ìš”)
        - ê¸°ì—…ëª…:(ê¸°ì—…ëª…)
        - íƒœê·¸:(íƒœê·¸)
        - ì£¼ìš” ì—…ë¬´: (ì£¼ìš” ì—…ë¬´ì— ëŒ€í•œ ì„¤ëª…)
        - ìê²© ìš”ê±´: (í•„ìˆ˜ ìê²© ìš”ê±´)
        - ê²½ë ¥: (ê²½ë ¥)

        3. ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì±„ìš© ì •ë³´ ì§ˆë¬¸ì„ ìœ ë„í•˜ì„¸ìš”.
        4. ì§ê¸‰ê³¼ ê´€ë ¨ëœ ë‹µë³€ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        ì§ˆë¬¸: {question}
        
        ê´€ë ¨ ë¬¸ì„œ:
        {context}
        """
    )

    return ConversationalRetrievalChain.from_llm(
        llm=chat_model,
        retriever=vector_store.as_retriever(),
        return_source_documents=True,
        verbose=True,
        combine_docs_chain_kwargs={"prompt": prompt_template}  # ì´ì œ PromptTemplate ê°ì²´ ì‚¬ìš©
    )

def get_answer(question, retrieval_chain):
    """ì±„ìš© ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    result = retrieval_chain.invoke({"question": question, "chat_history": []})

    answer = result.get("answer", "")
    logging.info(f"ğŸŸ¢ [ë‹µë³€ ìƒì„± ì™„ë£Œ] ë‹µë³€: {answer[:100]}...")  # ë„ˆë¬´ ê¸¸ë©´ ìë¦„

    return answer, result.get("source_documents", [])

# âœ… 3ï¸âƒ£ ì§ˆë¬¸-ë‹µë³€ ê²€ì¦ GPT
def verify_answer(question, answer):
    """ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ë§ëŠ”ì§€ ê²€ì¦"""
    system_prompt = """
    ë‹¹ì‹ ì€ AI ê²€ì¦ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ íë¦„ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”. íŒë³„ì„ 'ì¼ì¹˜'ë¡œ íŒë³„í•˜ì„¸ìš”

    - ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ë‚´ìš© íë¦„ì´ ì—°ê²°ë˜ë©´: {"verification": "ì¼ì¹˜"}
    - ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ë‚´ìš© íë¦„ì´ ì—°ê²°ë˜ì§€ ì•Šìœ¼ë©´: {"verification": "ë¶ˆì¼ì¹˜"}
    """
    ### ë¶ˆì¼ì¹˜ í•˜ëŠ” ê³¼ì •ì„ í™•ì¸. ë¶ˆì¼ì¹˜ ë¹„ìœ¨ ê²€ì¦. 

    response = openai.ChatCompletion.create(
        model="gpt-4o-mini", 
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ì§ˆë¬¸: {question}\në‹µë³€: {answer}"}
        ],
        temperature=0.3,
        max_tokens=50
    )

    result = json.loads(response["choices"][0]["message"]["content"])
    logging.info(f"ğŸŸ¢ [ë‹µë³€ ê²€ì¦ ì™„ë£Œ] ê²°ê³¼: {result['verification']}")
    return result

# âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
chat_history = []

def get_answer_with_similarity_check(question, vector_store, chat_history):
    """ì±„ìš© ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ë©°, ìœ ì‚¬ë„ê°€ 0.2 ì´ìƒì¸ì§€ í™•ì¸"""
    retriever = vector_store.as_retriever()
    print(retriever.vectorstore.index.metric_type)

    
    # ìœ ì‚¬ë„ ê²€ì‚¬ (ìµœëŒ€ 3ê°œì˜ ë¬¸ì„œ ê²€ìƒ‰)
    docs_with_scores = retriever.vectorstore.similarity_search_with_score(question, k=3)
    
    # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ë¬¸ì„œ ì°¾ê¸°
    if not docs_with_scores:
        logging.warning("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return "âš  ê´€ë ¨ëœ ì±„ìš© ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.", []
    
    highest_score = max(score for _, score in docs_with_scores)
    logging.info(f'âœ…ìœ ì‚¬ë„ ìµœê³  ì ìˆ˜: {highest_score}')

    # âœ… ìœ ì‚¬ë„ê°€ 0.3 ì´ìƒì´ë©´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
    if highest_score >= 0.3:
        retrieval_chain = create_chatbot(vector_store)
        result = retrieval_chain.invoke({
            "question": question, 
            "chat_history": chat_history  # âœ… ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
        })
        answer = result.get("answer", "")

        # âœ… ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        chat_history.append({"question": question, "answer": answer})
        
        return answer, result.get("source_documents", [])

    # âŒ ìœ ì‚¬ë„ê°€ ë‚®ì„ ê²½ìš° ë‹¤ì‹œ ì§ˆë¬¸ì„ ìœ ë„
    logging.warning("âŒ ìœ ì‚¬ë„ê°€ ë‚®ì•„ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return "âš  í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì±„ìš© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” ìƒì„¸íˆ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!", []


# âœ… 4ï¸âƒ£ ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜ (ëŒ€í™” ê¸°ë¡ í™œìš©)
def handle_request(question):
    """ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³ , ìœ ì‚¬ë„ ê¸°ì¤€ì„ ì ìš©í•˜ì—¬ ë°ì´í„° ê²€ìƒ‰"""
    
    # 1ï¸âƒ£ ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤í–‰
    category_result = classify_question(question)
    category = category_result["category"]
    
    if category == "ì¼ë°˜ ì§ˆë¬¸":
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system",
                "content": """ë‹¹ì‹ ì€ ì¡Jobì˜ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                            ë‹¹ì‹ ì€ ì±„ìš© ì •ë³´ì— ê´€í•œ ì§ˆë¬¸ì—ë§Œ ëŒ€ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ì™¸ì˜ ì§ˆë¬¸ì—ëŠ” í•œì¤„ì˜ ë‹¨ë‹µí˜•ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.
                            'ì±„ìš© ê³µê³ ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”?'ë¥¼ ë§ˆì§€ë§‰ì— ë§ë¶™ì´ì„¸ìš”"""},
                # âœ… ëŒ€í™” ê¸°ë¡ ì¶”ê°€
                *[
                    {"role": "user", "content": entry["question"]}
                    for entry in chat_history
                ],
                *[
                    {"role": "assistant", "content": entry["answer"]}
                    for entry in chat_history
                ],
                {"role": "user", "content": question}
            ],
            temperature=0.2,
            max_tokens=300
        )
        
        answer = response["choices"][0]["message"]["content"]
        
        # âœ… ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        chat_history.append({"question": question, "answer": answer})
        
        return answer

    # 2ï¸âƒ£ ì±„ìš© ì •ë³´ ê²€ìƒ‰ ì‹¤í–‰
    courses = fetch_from_saramin_refined()
    documents = convert_to_documents(courses)
    vector_store = create_vector_store(documents)

    # 2ï¸âƒ£.5ï¸âƒ£ ìœ ì‚¬ë„ ê²€ì‚¬ ë° ì‘ë‹µ ìƒì„± (ëŒ€í™” ê¸°ë¡ ì¶”ê°€)
    answer, source_docs = get_answer_with_similarity_check(question, vector_store, chat_history)

    # 3ï¸âƒ£ ì§ˆë¬¸-ë‹µë³€ ê²€ì¦ ì‹¤í–‰ (ìœ ì‚¬ë„ ê¸°ì¤€ í†µê³¼í•œ ê²½ìš°ë§Œ ê²€ì¦)
    if source_docs:
        verification_result = verify_answer(question, answer)
        if verification_result["verification"] == "ì¼ì¹˜":
            return answer  
        else:
        # ê²€ì¦ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš° ì‚¬ìš©ìì—ê²Œ ë‹¤ì‹œ ì§ˆë¬¸ì„ ìœ ë„í•˜ëŠ” ë©”ì‹œì§€ ë°˜í™˜
            return "ë‹µë³€ì´ ì •í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
        

    return answer  # ìœ ì‚¬ë„ ë‚®ì„ ê²½ìš° ì§ˆë¬¸ ìœ ë„ ë©”ì‹œì§€ ë°˜í™˜

# âœ… ì‹¤í–‰ ì˜ˆì œ
# if __name__ == "__main__":
#     test_question = "ì‹í’ˆê³µí•™ì „ê³µ ê³µê³  ì¶”ì²œ."
#     print(handle_request(test_question))



# Streamlit UI
st.set_page_config(page_title="AI Chatbot", layout="wide")

# ì‚¬ì´ë“œë°” (ë¡œê·¸ ì €ì¥)
# st.sidebar.title("ğŸ” ê²€ìƒ‰ ë¡œê·¸")

# if "logs" not in st.session_state:
#     st.session_state.logs = []

# ###
# # log_print ë¦¬ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
# if "log_print" in locals() and log_print:  
#     st.session_state.logs.extend(log_print)
#     log_print.clear()  # log_printë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ì¤‘ë³µ ì €ì¥ ë°©ì§€

# # ì‚¬ì´ë“œë°”ì— ë¡œê·¸ ì¶œë ¥ (í•œ ì¤„ì”©)
# for log in st.session_state.logs:
#     st.sidebar.text(log)
###
###
# for log in st.session_state.logs:
#     st.sidebar.text(log)


###

# ë©”ì¸ ì±— ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ’¬ ì¡Job ì±„ìš© ê³µê³  ì¶”ì²œ ì±—ë´‡")
if "messages" not in st.session_state:
    st.session_state.messages = []

if "conversation_history" in st.session_state:
    # conversation_historyê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ í›„ ë°˜ë³µ
    if isinstance(st.session_state.conversation_history, list):
        for message in st.session_state.conversation_history:
            if isinstance(message, dict) and "assistant" in message:
                if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
                    # ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— assistant ì‘ë‹µ ì¶”ê°€
                    st.session_state.messages[-1]["content"] += f"\n\n{message['assistant']}"
                else:
                    # ìƒˆë¡œìš´ assistant ë©”ì‹œì§€ ì¶”ê°€
                    st.session_state.messages.append({"role": "assistant", "content": message["assistant"]})



# ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if 'conversation_history' not in st.session_state:
    st.session_state.conversation_history = []

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.session_state.conversation_history.append(f"User: {user_input}")
    with st.chat_message("user"):
        st.markdown(user_input)
    # st.write(st.session_state.messages)
    
    # FAISS ê²€ìƒ‰ ìˆ˜í–‰
    data = fetch_from_saramin_refined()
    documents = convert_to_documents(data)
    vector_store = create_vector_store(documents)
    chatbot = create_chatbot(vector_store)
    st.session_state["vector_store"] = vector_store
    st.session_state["chatbot"] = chatbot

    # index, ids = build_faiss_index(data)
    # relevant_ids = search_faiss(user_input, index, ids)
    # st.session_state.logs.append(f"ğŸ”¹ ê²€ìƒ‰ëœ ë¬¸ì„œ ID: {relevant_ids}")
    
    # OpenAI ì‘ë‹µ ìƒì„±
    response_text = handle_request(user_input)
    st.session_state.messages.append({"role": "assistant", "content": response_text})
    st.session_state.conversation_history.append(f"assistant: {response_text}")
    
    # AI ì‘ë‹µ í‘œì‹œ
    with st.chat_message("assistant"):
        st.markdown(response_text)

    #for message in st.session_state.conversation_history:
        # st.write(message)