import logging
from dotenv import load_dotenv
import os
from datetime import datetime

# DB
import pymysql
import faiss
import numpy as np

# ChatBot
import openai
import streamlit as st
import json

# LangChain
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

# ë¡œê·¸ ì„¤ì •
timestamp = datetime.now().strftime(r"%Y-%m-%d %H:%M:%S")
logging.basicConfig(level=logging.INFO, format=r"%(asctime)s - %(levelname)s - %(message)s")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# MySQL ì—°ê²° ì •ë³´
MYSQL_USER = os.getenv("MYSQL_USER")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD")

log_print = []

#################
##   db ì—°ê²°   ##
#################
def connect_to_db():
    """MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    return pymysql.connect(
        host='127.0.0.1',
        port=3306,
        user='root',
        passwd='seul0899',
        db='saramin',
        charset="utf8mb4"
    )

def fetch_from_saramin_refined():
    """MySQLì—ì„œ ì±„ìš© ì •ë³´ë¥¼ ê°€ì ¸ì˜´"""
    connection = connect_to_db()
    cursor = connection.cursor(pymysql.cursors.DictCursor)
    cursor.execute("SELECT * FROM saramin_5_combined_revised_record_fordb")
    fetch_result = cursor.fetchall()
    connection.close()
    
    logging.info(f"ğŸŸ¢ [DB ë°ì´í„° ë¡œë“œ ì™„ë£Œ] ì´ {len(fetch_result)}ê°œ ë¬¸ì„œ ê°€ì ¸ì˜´")
    return fetch_result

def convert_to_documents(courses):
    """MySQL ë°ì´í„°ë¥¼ LangChain ë¬¸ì„œë¡œ ë³€í™˜ (ì²­í¬ ì—†ìŒ)"""
    documents = []
    for course in courses:
        doc_text = "\n".join([f"{key}: {value}" for key, value in course.items()])
        documents.append(Document(page_content=doc_text))  # ì²­í¬ ì—†ì´ ì „ì²´ ë¬¸ì„œ ì €ì¥

    logging.info(f"ğŸŸ¢ [ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ] ì´ {len(documents)}ê°œ ë¬¸ì„œ ìƒì„±ë¨")
    return documents

def normalize_vector_store(store):
    """FAISS ë²¡í„°ë“¤ì„ ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ê²Œ ë³€ê²½"""
    xb = store.index.reconstruct_n(0, store.index.ntotal)  # ëª¨ë“  ë²¡í„° ê°€ì ¸ì˜¤ê¸°
    xb /= np.linalg.norm(xb, axis=1, keepdims=True)  # ë²¡í„° ì •ê·œí™”
    
    # ì •ê·œí™”ëœ ë²¡í„°ì˜ L2 ë…¸ë¦„ í™•ì¸
    norm = np.linalg.norm(xb, axis=1)
    if np.allclose(norm, 1.0):
        print("ë²¡í„°ëŠ” ëª¨ë‘ ì •ê·œí™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ë²¡í„°ê°€ ì¼ë¶€ ì •ê·œí™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    store.index = faiss.IndexFlatIP(xb.shape[1])  # ë‚´ì  ê¸°ë°˜ ì¸ë±ìŠ¤ ë‹¤ì‹œ ìƒì„±
    store.index.add(xb)  # ì •ê·œí™”ëœ ë²¡í„° ì¶”ê°€
    # ìƒˆë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ì˜ metric_type ì¶œë ¥
    print("ìƒˆë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ì˜ metric_type:", store.index.metric_type)
    return store

def create_vector_store(_documents):
    """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì €ì¥"""
    embeddings = OpenAIEmbeddings()
    faiss_path = './db/faiss_norm'  # ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ

    # âœ… ë²¡í„° ìŠ¤í† ì–´ê°€ ì¡´ì¬í•˜ë©´ ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(faiss_path):
        logging.info("ğŸŸ¡ [ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ] ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        store = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)
    else:
        # âŒ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ìƒì„± í›„ ì €ì¥
        logging.info("ğŸ”µ [ë²¡í„° ìŠ¤í† ì–´ ìƒì„±] ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë§Œë“­ë‹ˆë‹¤.")
        store = FAISS.from_documents(documents, embeddings)
        # ğŸ”¹ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìš©: ë²¡í„° ì •ê·œí™”
        store = normalize_vector_store(store)
        store.save_local(faiss_path)  # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        logging.info("ğŸŸ¢ [ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ] ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    logging.info(f"ğŸŸ¢ [ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ] ë¬¸ì„œ ê°œìˆ˜: {len(documents)}")
    
    return store

#################
##   ì±—ë´‡ ìƒì„±  ##
#################


# âœ…ì§ˆë¬¸ ë¶„ë¥˜ GPT
def classify_question(question):
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ” GPT"""
    logging.info(f"ğŸŸ¡ [ì§ˆë¬¸ ë¶„ë¥˜] ì…ë ¥ ì§ˆë¬¸: {question}")
    log_print.append(f"{timestamp}ğŸŸ¡ [ì§ˆë¬¸ ë¶„ë¥˜] ì…ë ¥ ì§ˆë¬¸: {question}")

    system_prompt = """
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤.
    ì§ˆë¬¸ì„ 1.ì±„ìš© ê³µê³  í˜•ì‹ì´ í•„ìš”í•œ ì§ˆë¬¸(=ì±„ìš©ì •ë³´)ê³¼ 2.ì±„ìš© ê³µê³  í˜•ì‹ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸(=ì¼ë°˜ì§ˆë¬¸), ì´ ë‘ê°€ì§€ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    
    ì±„ìš© ê³µê³  í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    [ì±„ìš© ê³µê³  í˜•ì‹]
    ê³µê³  ì œëª©: (ê³µê³ ì œëª©)
    ê³µê³  ë§í¬: (ê³µê³ ë§í¬ë¥¼ ê·¸ëŒ€ë¡œ ì œê³µí•˜ì„¸ìš”)
    ê¸°ì—…ëª…:(ê¸°ì—…ëª…)
    íƒœê·¸:(íƒœê·¸)
    ì£¼ìš” ì—…ë¬´: (ì£¼ìš” ì—…ë¬´ì— ëŒ€í•œ ì„¤ëª…)
    ìê²© ìš”ê±´: (í•„ìˆ˜ ìê²© ìš”ê±´)
    ê²½ë ¥: (ê²½ë ¥)
    
    **ì¶œë ¥ í˜•ì‹ (JSON)**
    {"category": "ì±„ìš© ì •ë³´"}
    {"category": "ì¼ë°˜ ì§ˆë¬¸"}
    """
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ],
        temperature=0.3,
        max_tokens=50
    )
    result = json.loads(response["choices"][0]["message"]["content"])
    logging.info(f"ğŸŸ¢ [ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ] ì¹´í…Œê³ ë¦¬: {result['category']}")
    return result  # JSON ë³€í™˜

def create_chatbot_for_job(vector_store):
    """ì±„ìš© ì •ë³´ ë‹µë³€ìš© ì±—ë´‡ ìƒì„±"""
    chat_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

    prompt_template = PromptTemplate(
        input_variables=["question", "context",'chat_history'],  # í•„ìš”í•œ ë³€ìˆ˜ ì •ì˜
        template="""
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:
        1. ë‹¹ì‹ ì€ ì¡Jobì˜ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤.ì°¸ì¡° ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ í˜•ì‹ì— ë§ê²Œ ì±„ìš© ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
            [ì±„ìš© ê³µê³  í˜•ì‹]
            - ê³µê³  ì œëª©: (ê³µê³ ì œëª©)
            - ê³µê³  ë§í¬: (ê³µê³ ë§í¬ë¥¼ ê·¸ëŒ€ë¡œ ì œê³µí•˜ì„¸ìš”)
            - ê¸°ì—…ëª…:(ê¸°ì—…ëª…)
            - íƒœê·¸:(íƒœê·¸)
            - ì£¼ìš” ì—…ë¬´: (ì£¼ìš” ì—…ë¬´ì— ëŒ€í•œ ì„¤ëª…)
            - ìê²© ìš”ê±´: (í•„ìˆ˜ ìê²© ìš”ê±´)
            - ê²½ë ¥: (ê²½ë ¥)
        3. ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì±„ìš© ì •ë³´ ì§ˆë¬¸ì„ ìœ ë„í•˜ì„¸ìš”.
        4. ì§ê¸‰ê³¼ ê´€ë ¨ëœ ë‹µë³€ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        ëŒ€í™”ê¸°ë¡: {chat_history}

        ì§ˆë¬¸: {question}
        
        ê´€ë ¨ ë¬¸ì„œ:
        {context}
        """
    )
    return ConversationalRetrievalChain.from_llm(
        llm=chat_model,
        retriever=vector_store.as_retriever(),
        return_source_documents=True,
        verbose=True,
        combine_docs_chain_kwargs={"prompt": prompt_template}  # ì´ì œ PromptTemplate ê°ì²´ ì‚¬ìš©
    )
    
def create_chatbot_for_normal(vector_store):
    """ì¼ë°˜ ì§ˆë¬¸ ë‹µë³€ìš© ì±—ë´‡ ìƒì„±"""
    chat_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

    prompt_template = PromptTemplate(
        input_variables=["question", "context",'chat_history'],  # í•„ìš”í•œ ë³€ìˆ˜ ì •ì˜
        template="""
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:
        ë‹¹ì‹ ì€ jobì¡ì˜ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì±„ìš©ê³µê³ ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì—ë§Œ ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
        ë°˜ë“œì‹œ ê´€ë ¨ ë¬¸ì„œì—ì„œë§Œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”. 

        ëŒ€í™”ê¸°ë¡: {chat_history}

        ì§ˆë¬¸: {question}
        
        ê´€ë ¨ ë¬¸ì„œ:
        {context}
        """
    )

    return ConversationalRetrievalChain.from_llm(
        llm=chat_model,
        retriever=vector_store.as_retriever(),
        return_source_documents=True,
        verbose=True,
        combine_docs_chain_kwargs={"prompt": prompt_template}  # ì´ì œ PromptTemplate ê°ì²´ ì‚¬ìš©
    )

# âœ… ì§ˆë¬¸-ë‹µë³€ ê²€ì¦ GPT
def verify_answer(question, answer):
    """ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ë§ëŠ”ì§€ ê²€ì¦"""
    system_prompt = """
    ë‹¹ì‹ ì€ AI ê²€ì¦ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 
    ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ íë¦„ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”. 
    ì‚¬ìš©ìì˜ ì¸ì‚¬ëŠ” 'ì¼ì¹˜'ë¡œ íŒë³„í•˜ì„¸ìš”

    - ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ë‚´ìš© íë¦„ì´ ì—°ê²°ë˜ë©´: {"verification": "ì¼ì¹˜"}
    - ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ë‚´ìš© íë¦„ì´ ì—°ê²°ë˜ì§€ ì•Šìœ¼ë©´: {"verification": "ë¶ˆì¼ì¹˜"}
    """
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini", 
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ì§ˆë¬¸: {question}\në‹µë³€: {answer}"}
        ],
        temperature=0.3,
        max_tokens=50
    )
    verify_result = json.loads(response["choices"][0]["message"]["content"])
    logging.info(f"ğŸŸ¢ [ë‹µë³€ ê²€ì¦ ì™„ë£Œ] ê²°ê³¼: {verify_result['verification']}")
    return verify_result

logging.basicConfig(level=logging.INFO)

######################
###    ë‹µë³€ ìƒì„±    ###
######################

def format_chat_history(chat_history):
    """LangChainì´ ì§€ì›í•˜ëŠ” (role, content) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if isinstance(chat_history, list) and all(isinstance(entry, dict) for entry in chat_history):
        return [(entry["role"], entry["content"]) for entry in chat_history]
    return chat_history  # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜


def get_answer_with_similarity_check(question, vector_store, chat_history, category):
    """ì±„ìš© ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ë©°, ìœ ì‚¬ë„ 0.7 ì´ìƒì¸ ë¬¸ì„œë§Œ ì‚¬ìš©"""
    
    formatted_chat_history = format_chat_history(chat_history)
    retriever = vector_store.as_retriever()
    
    print(retriever.vectorstore.index.metric_type)
    
    # 1ï¸âƒ£ ì§ˆë¬¸ê³¼ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì§ˆë¬¸ ìƒì„±
    combined_question = question + " " + " ".join(
        [entry["question"] + " " + entry["answer"] for entry in chat_history[-3:]]
    )

    # 2ï¸âƒ£ ìœ ì‚¬ë„ ê²€ì‚¬ (ìµœëŒ€ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰)
    docs_with_scores = retriever.vectorstore.similarity_search_with_score(combined_question, k=5)
    
    # 3ï¸âƒ£ ìœ ì‚¬ë„ 0.7 ì´ìƒì¸ ë¬¸ì„œë§Œ í•„í„°ë§
    filtered_docs = [doc for doc, score in docs_with_scores if score >= 0.7]
    
    # âœ… í•„í„°ë§ëœ ë¬¸ì„œ ë¡œê·¸ ì¶œë ¥
    logging.info(f"âœ… 0.7 ì´ìƒì¸ ë¬¸ì„œ ê°œìˆ˜: {len(filtered_docs)}")
    
    if not filtered_docs:
        logging.warning("âŒ ìœ ì‚¬ë„ 0.7 ì´ìƒì¸ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return "âš  ê´€ë ¨ëœ ì±„ìš© ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.", []

    # 4ï¸âƒ£ í•„í„°ë§ëœ ë¬¸ì„œë§Œ contextë¡œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    if category == 'ì±„ìš© ì •ë³´':
        retrieval_chain = create_chatbot_for_job(vector_store)
    else:
        retrieval_chain = create_chatbot_for_normal(vector_store)

    result = retrieval_chain.invoke({
        "question": question,
        "chat_history": formatted_chat_history,
        "context": "\n\n".join([doc.page_content for doc in filtered_docs])  # 0.7 ì´ìƒ ë¬¸ì„œë§Œ contextë¡œ ì „ë‹¬
    })
    
    answer = result.get("answer", "")

    # âœ… ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
    chat_history.append({"question": question, "answer": answer})
    print('í˜„ì¬ ë°˜ì˜ëœ chat_history:', chat_history)
    logging.info(f"Updated chat_history: {chat_history}")

    return answer, filtered_docs




def handle_request(question):
    """ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³ , ìœ ì‚¬ë„ ê¸°ì¤€ì„ ì ìš©í•˜ì—¬ ë°ì´í„° ê²€ìƒ‰"""
    
    # âœ… ì„¸ì…˜ì—ì„œ chat_history ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”)
    chat_history = st.session_state.get("chat_history", [])

    formatted_chat_history = format_chat_history(chat_history)
    
    # 1ï¸âƒ£ ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤í–‰
    category_result = classify_question(question)
    category = category_result["category"]

    vector_store = st.session_state.get("vector_store")
    if vector_store is None:
        data = fetch_from_saramin_refined()
        documents = convert_to_documents(data)
        vector_store = create_vector_store(documents)
        st.session_state["vector_store"] = vector_store
        
    # âœ… ìœ ì‚¬ë„ ê²€ì‚¬ ë° ì‘ë‹µ ìƒì„±
    answer, source_docs = get_answer_with_similarity_check(question, vector_store, formatted_chat_history, category)

    if source_docs:
        verification_result = verify_answer(question, answer)
        if verification_result["verification"] == "ì¼ì¹˜":
            return answer
        else:
            return "ë‹µë³€ì´ ì •í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."


    return answer



# Streamlit UI
st.set_page_config(page_title="AI Chatbot", layout="wide")

# ë©”ì¸ ì±— ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ’¬ ì¡Job ì±„ìš© ê³µê³  ì¶”ì²œ ì±—ë´‡")
if "messages" not in st.session_state:
    st.session_state.messages = []

if "conversation_history" in st.session_state:
    # conversation_historyê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ í›„ ë°˜ë³µ
    if isinstance(st.session_state.conversation_history, list):
        for message in st.session_state.conversation_history:
            if isinstance(message, dict) and "assistant" in message:
                if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
                    # ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— assistant ì‘ë‹µ ì¶”ê°€
                    st.session_state.messages[-1]["content"] += f"\n\n{message['assistant']}"
                else:
                    # ìƒˆë¡œìš´ assistant ë©”ì‹œì§€ ì¶”ê°€
                    st.session_state.messages.append({"role": "assistant", "content": message["assistant"]})


# ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if 'conversation_history' not in st.session_state:
    st.session_state.conversation_history = []

if 'chat_history' not in st.session_state:  # ğŸš€ chat_historyë¥¼ session_stateì—ì„œ ê´€ë¦¬
    st.session_state.chat_history = []

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.session_state.conversation_history.append(f"User: {user_input}")
    st.session_state.chat_history.append({"role": "user", "content": user_input})  # âœ… ì¶”ê°€

    with st.chat_message("user"):
        st.markdown(user_input)

    # FAISS ê²€ìƒ‰ ìˆ˜í–‰
    data = fetch_from_saramin_refined()
    documents = convert_to_documents(data)
    vector_store = create_vector_store(documents)
    chatbot = create_chatbot_for_job(vector_store)
    st.session_state["vector_store"] = vector_store
    st.session_state["chatbot"] = chatbot

    # OpenAI ì‘ë‹µ ìƒì„±
    response_text = handle_request(user_input)  # âœ… ëŒ€í™” ê¸°ë¡ ìœ ì§€
    st.session_state.messages.append({"role": "assistant", "content": response_text})
    st.session_state.conversation_history.append(f"assistant: {response_text}")
    st.session_state.chat_history.append({"role": "assistant", "content": response_text})  # âœ… ì¶”ê°€

    # AI ì‘ë‹µ í‘œì‹œ
    with st.chat_message("assistant"):
        st.markdown(response_text)